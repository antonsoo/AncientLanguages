"""Initialize LDSv1 schema with extensions and 3 GIN indexes

Revision ID: 8144e6c26432
Revises:
Create Date: 2025-09-02 16:24:23.412566

"""

from typing import Sequence, Union

import pgvector.sqlalchemy
import sqlalchemy as sa
from alembic import op
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "8144e6c26432"
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # --- MANUAL ADDITIONS START (Extensions) ---
    op.execute("CREATE EXTENSION IF NOT EXISTS vector")
    op.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm")
    # --- MANUAL ADDITIONS END ---

    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "language",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("code", sa.String(length=8), nullable=False),
        sa.Column("name", sa.String(length=64), nullable=False),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(op.f("ix_language_code"), "language", ["code"], unique=True)
    op.create_table(
        "source_doc",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("slug", sa.String(length=64), nullable=False),
        sa.Column("title", sa.String(length=256), nullable=False),
        sa.Column("license", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column("meta", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("slug"),
    )
    op.create_table(
        "grammar_topic",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("source_id", sa.Integer(), nullable=False),
        sa.Column("anchor", sa.String(length=64), nullable=False),
        sa.Column("title", sa.String(length=256), nullable=False),
        sa.Column("body", sa.Text(), nullable=False),
        sa.Column("body_fold", sa.Text(), nullable=False),
        sa.Column("emb", pgvector.sqlalchemy.vector.VECTOR(dim=1536), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["source_id"],
            ["source_doc.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "lexeme",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("language_id", sa.Integer(), nullable=False),
        sa.Column("lemma", sa.String(length=150), nullable=False),
        sa.Column("lemma_fold", sa.String(length=150), nullable=False),
        sa.Column("pos", sa.String(length=32), nullable=True),
        sa.Column("data", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["language_id"],
            ["language.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("language_id", "lemma", name="uq_lex_lang_lemma"),
    )
    op.create_index("ix_lexeme_lang_lemma", "lexeme", ["language_id", "lemma"], unique=False)
    op.create_index(op.f("ix_lexeme_language_id"), "lexeme", ["language_id"], unique=False)
    op.create_index(op.f("ix_lexeme_lemma"), "lexeme", ["lemma"], unique=False)
    op.create_index(op.f("ix_lexeme_lemma_fold"), "lexeme", ["lemma_fold"], unique=False)
    op.create_table(
        "text_work",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("language_id", sa.Integer(), nullable=False),
        sa.Column("source_id", sa.Integer(), nullable=False),
        sa.Column("author", sa.String(length=128), nullable=False),
        sa.Column("title", sa.String(length=256), nullable=False),
        sa.Column("ref_scheme", sa.String(length=64), nullable=False),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["language_id"],
            ["language.id"],
        ),
        sa.ForeignKeyConstraint(
            ["source_id"],
            ["source_doc.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(op.f("ix_text_work_language_id"), "text_work", ["language_id"], unique=False)
    op.create_table(
        "text_segment",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("work_id", sa.Integer(), nullable=False),
        sa.Column("ref", sa.String(length=64), nullable=False),
        sa.Column("text_raw", sa.Text(), nullable=False),
        sa.Column("text_nfc", sa.Text(), nullable=False),
        sa.Column("text_fold", sa.Text(), nullable=False),
        sa.Column("emb", pgvector.sqlalchemy.vector.VECTOR(dim=1536), nullable=True),
        sa.Column("meta", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["work_id"],
            ["text_work.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("work_id", "ref", name="uq_segment_ref"),
    )
    op.create_index(op.f("ix_text_segment_work_id"), "text_segment", ["work_id"], unique=False)
    op.create_table(
        "token",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("segment_id", sa.Integer(), nullable=False),
        sa.Column("idx", sa.Integer(), nullable=False),
        sa.Column("surface", sa.String(length=150), nullable=False),
        sa.Column("surface_nfc", sa.String(length=150), nullable=False),
        sa.Column("surface_fold", sa.String(length=150), nullable=False),
        sa.Column("lemma", sa.String(length=150), nullable=True),
        sa.Column("lemma_fold", sa.String(length=150), nullable=True),
        sa.Column("msd", postgresql.JSONB(astext_type=sa.Text()), nullable=True),
        sa.Column(
            "created_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            server_default=sa.text("now()"),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["segment_id"],
            ["text_segment.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(op.f("ix_token_lemma"), "token", ["lemma"], unique=False)
    op.create_index(op.f("ix_token_lemma_fold"), "token", ["lemma_fold"], unique=False)
    op.create_index("ix_token_loc", "token", ["segment_id", "idx"], unique=False)
    op.create_index(op.f("ix_token_segment_id"), "token", ["segment_id"], unique=False)
    op.create_index(op.f("ix_token_surface_fold"), "token", ["surface_fold"], unique=False)
    op.create_index(op.f("ix_token_surface_nfc"), "token", ["surface_nfc"], unique=False)

    # --- MANUAL ADDITIONS START (GIN Indexes) ---
    # CRITICAL: Ensure these are placed AFTER the corresponding op.create_table calls.

    # Index 1: Token surface forms
    op.create_index(
        "ix_token_surface_fold_trgm",
        "token",
        ["surface_fold"],
        postgresql_using="gin",
        postgresql_ops={"surface_fold": "gin_trgm_ops"},
    )
    # Index 2: Lexeme lemmas
    op.create_index(
        "ix_lexeme_lemma_fold_trgm",
        "lexeme",
        ["lemma_fold"],
        postgresql_using="gin",
        postgresql_ops={"lemma_fold": "gin_trgm_ops"},
    )
    # Index 3: Text Segment content (The critical addition)
    op.create_index(
        "ix_text_segment_text_fold_trgm",
        "text_segment",
        ["text_fold"],
        postgresql_using="gin",
        postgresql_ops={"text_fold": "gin_trgm_ops"},
    )
    # --- MANUAL ADDITIONS END ---
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # --- MANUAL ADDITIONS START (GIN Indexes) ---
    op.drop_index(
        "ix_text_segment_text_fold_trgm",
        table_name="text_segment",
        postgresql_using="gin",
    )
    op.drop_index("ix_lexeme_lemma_fold_trgm", table_name="lexeme", postgresql_using="gin")
    op.drop_index("ix_token_surface_fold_trgm", table_name="token", postgresql_using="gin")
    # --- MANUAL ADDITIONS END ---

    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index(op.f("ix_token_surface_nfc"), table_name="token")
    op.drop_index(op.f("ix_token_surface_fold"), table_name="token")
    op.drop_index(op.f("ix_token_segment_id"), table_name="token")
    op.drop_index("ix_token_loc", table_name="token")
    op.drop_index(op.f("ix_token_lemma_fold"), table_name="token")
    op.drop_index(op.f("ix_token_lemma"), table_name="token")
    op.drop_table("token")
    op.drop_index(op.f("ix_text_segment_work_id"), table_name="text_segment")
    op.drop_table("text_segment")
    op.drop_index(op.f("ix_text_work_language_id"), table_name="text_work")
    op.drop_table("text_work")
    op.drop_index(op.f("ix_lexeme_lemma_fold"), table_name="lexeme")
    op.drop_index(op.f("ix_lexeme_lemma"), table_name="lexeme")
    op.drop_index(op.f("ix_lexeme_language_id"), table_name="lexeme")
    op.drop_index("ix_lexeme_lang_lemma", table_name="lexeme")
    op.drop_table("lexeme")
    op.drop_table("grammar_topic")
    op.drop_table("source_doc")
    op.drop_index(op.f("ix_language_code"), table_name="language")
    op.drop_table("language")
    # ### end Alembic commands ###
